---
title: Chapter 12. Scalability and Architectural Patterns
sidebar_position: 12
---

Разглеждаме подходи за скалиране на една Node.js апликация. По принцип има два основни начина да скалираме каквото и да е - вертикално и хоризонтално. Вертикалното скалиране се състои в това да се подобрят способносите на определен сървър чрез хардуерни модификации, но поради факта, че JS е single-threaded език, за да се възползваме от повече CPU power/cores ние всъщност трябва да отворим нови процеси, което се равнява на хоризонтално скалиране. В тази глава, обаче, разглеждаме техники за скалиране, които са малко конкретни. Имаме три измерения на скалиране, които са описани в "scale cube" модела:

- X-Axis - Хоризонтално клониране или използване на Н на брой идентични инстанции, които седят зад load balancer.
- Y-Axis - Функционална декомпозиция. Това се състои в разделянето на апликацията на няколко отделни services, всяка от която има отделна код база. Това е така нареченият microservice модел.
- Z-Axis - Data partitioning, тоест разделяне на данните, така че всяка инстанция е отговорна само за събсет от данните (например разделяне на потребителите по регион). За да се установи това скалиране ни трябва и routing layer, който да пренасочва заявките към правилната инстанция.
  По принцип, когато се скалира една апликация първо се прилагат X & Y axis scaling и след това, когато сме изчерпали тези подходи пристъпваме към data partitioning.

Започваме с X-Axis Scaling - най-лесният начин да клонираме една инстанция в Node.js е да се възползваме от `cluster` модула. Той ни позволява да имаме един мастер порцес, който можем да "форк"-нем на множество worker процеси и автоматично да балансира заявките измежду тях, чрез round-robin алгоритъм. Най-често ние бихме форкнали толкова сървъри, колкото CPU ядра имаме. След като вече имаме хоризонтално скалиран сървър трябва да помислим как ще прилагаме промени без да прекъсваме изцяло работата и без да създаваме изцлишен downtime. Тук решението е просто - трябва да рестартираме всеки процес по отделно и последователно, така че винаги да имаме възможност някои от инстанциите да обработи входяща заявка. За сега всичко е наред, но забравяме, че за да бъде една апликация функционална трябва да борави със стейт. По принцип когато имаме хоризонтално скалиране, най-добре е да прилагаме техники за споделяне на стейт като например обща база данни и кеш или директно кодиране данните чрез JWT токен. В случаите, когато трябва да пазим стейт в паметта на процеса можем да приложим така нареченият stick session подход, тоест трябва да конфигурираме load balancer-a ни, така че за всеки потребител да има инстанция, към която той е винаги пренасочван. Този подход не е предпоръчителен, защото до голяма степен поставя под риск цялата система (например ако определена инстанция дропне нейните потребители остават без възможност да използват нашата апликация, съответно има механизми за справяне с този проблем...).

Към този момент X-Axis scaling-a ни се състои в това да клонираме мноцество порцеси на една машина, но всъщност ние можем да имаме и няколко сървъра на различни машини. Тук идеята е да имаме сървъри, които са отворени с private IP и още един сървър, който наричаме Reverse Proxy, той от своя страна е отворен c public IP и има за задача да приема заявки и да ги препраща на нашите сървъри, тоест reverse proxy-то е външен load balancer. Пример за такъв сървър би бил Nginx.

Скалирането с reverse proxy е добър подход в много случаи, но той предполага точно определен брой сървъри, които не се променят, освен ако не променим конфигурацията. В много случаи обаче ние бихме искали динамично да променяме броя на сървърите ни според нуждите ни. Например в час пик ние искаме 20 инстанции в останалото време 5, по този начин ние имаме най-ефективно използване на ресурси. В такива ситуации трябва да използваме service discovery, което да ни казва колко и кои са инстанциите към които имаме достъп. Най-често използваме service registry, което е просто една база данни, която ние менажираме. Когато един сървър се стартира се добавя в базата, когато бъде затворен се премахва или се слага някакъв флаг. На практика бихме използвали нещо като Consul, което да ни служи като service registry-то и бихме пуснали един сървър, който да ни служи като reverse proxy, който ще се консултира с Consul и ще прави съответните решения.

Друго приложение на service registry-то би било вместо да имаме дедикиран load balancer да предоставяме на клиентите ни сами да си избират инстанцията към която ще направят заявка. Това се нарича peer-to-peer или client-side load balancing, един много добър пример за къде се използва този модел е при DNS. Когато прилагаме такава стратегия ние премахваме SPF-a, който е reverse proxy-то и симплифицираме процеса на backend-a, но задължаваме клиентите да имплементират собствен load balancing механизъм.

Всички техники към момента са валидни, но в днешно време стандарта за пакетиране и deploy-ване на апликации е чрез така наречените контейнери. В Docker контрейнерът е единица, която пакетира source code-a и неговите dependencies така че всяка апликация да може да бъде изпълнена в каквато и да е обстановка стига да има container runtime. Чрез Docker ние можем по много лесен начин да стартираме няколко инстанции на една Node.js програма, трябва да напишем Dockerfile, да го build-нем и да го стартираме, като можем да стартираме множество контейнери, които да слушат на различни портове. Това е сравнително семпъл процес, но с нарастването на компонентите в нашата система менажирането на контейнери става все по-сложно, за това ни трябва начин да "оркестрираме". Това правим чрез Kubernetes, с тази технология ние можем да автоматизираме deployment & scaling процеса, в много случаи само и единиствено чрез правилна конфигурация. Kubernetes ни помага с cluster management (комбинира нашите node-ове/машини и ни позволява динамичен scaling), self-healing (При непредвиден дроп на контейнер, той ще бъде засечен и процеса на замяна ще бъде активиран), service discovery & load balancing и други.

Време е да разгледаме и Y-Axis scaling-a. Той се състои в декомпозиране на апликацията на отделни по-малки компоненти, които имат точно определена цел. Това е познатата microservice архитектура. При стандартният монолит цялата (или по-голямата част) логика на апликацията се намира на едно място в една код база, това го прави по-лесен за първоначална имплементация и deployment. С течени на времето обаче апликацията се разраства и става все по-трудна да се поддържа и скалира и всяка малка промяна води со redeployment на цялата апликация. От друга страна когато приложим microservice архитектура различните части са сравнително самостоятелни, това означава, че могат да се поддържат по отделни и също така да се скалират по отделно. Самите services комуникират помежду си чрез HTTP, gRPC или message queues, което добавя известна доза latency и complexity, но в повечето случаи е поносимо. Решението между монолитна и microservice архитектура трябва да се направи според ситуацията и нуждите на апликацията и екипа, който я разработва - има много tradeoffs, които трябва да се предвидят.

На практика при microservice архитектурата, апликацията освен, че трябва да се раздели на отдели services, то те трябва и да комуникират и взимодействат по определен начин. Има няколко варианта да изградим такъв механизъм. Първо ще разгледаме API Gateway-a, това е общо казано специализирано reverse proxy на ниво апликация, идеята е да клиентите да правят заявки към апликацията, те да минават през gateway-a и от там да се route-ват към похдодящият microservice. В някои случаи можем да пердоставим публичен достъп до определена функционалност, която да има нужда от различни services да комуникират, да предоставят данни и те да се агрегират и т.н. API Gateway-a може да се счита като API Orchestration Layer, който "оркестрира" интеракциите между отделни services. Този подход обаче не винаги е походящ защото води то tight coupling и изгражда в някои случаи нездравослоно dependency.

Вторият начин за комуникация между services е чрез Message Brokers, това е event-driven подход или publish/subscribe модел. При този начин на комуникация ние имаме services, които публикуват (publish) евенти или съобщения, които обявяват какво се е случило (producers) и имаме services, които се абонират (subscribe) за тези евенти и реагират на тях (consumers). Това олицетворението на Observer pattern-a. Можем да използваме различни технологии, които са установени на пазара като RabbitMQ & Kafka, които ни предоставят много повече функционалности и възможности.
