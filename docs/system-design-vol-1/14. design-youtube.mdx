---
title: Chapter 14. Design YouTube
sidebar_position: 14
---

Така правим дизайн на YouTube XDDD от AliExpress. Имаме следните изисквания:

- Имаме два основни feature-a да качваме видеа и да гледаме видеа
- Трябва да поддържаме различни устройства web, mobile & smart TV
- Ще имаме 5М DAU
- Трябва да сме на международният пазар
- Трябва да поддържаме повечето познати видео формати в различни резолюции
- BOE Анализ:
  - Преполагаме, че всеки потребител би гледал по 5 видеа на ден
  - Преполагаме, че средно размерът на едно видео е 300MB
  - Преполагаме, че средно един CDN би ни таксувал 2 цента / 1GB трафик
  - Предполагаме, че 10% от потребителите в даден ден качват видео
  - Тоест от към пространство ще имаме нужда от 5М _ 0.1 _ 300 = 150TB ежедневно
  - Тоест от към финансов ресурс ще имаме нужда от 5М _ 5 _ 0.3 \* 0.02 = 150 000 USD всеки ден

Този анализ ни покзва слабите ни места и в последствие частите, които бихме искали да оптимизираме.

Започваме с изграждане на основите, не знам, нека започнем по модули/компоненти:

1. Клиентите са ясни, те комуникрат желанията на потребителя към API-то.
2. API Server - Това ни е задължителният дистрибутиран сървър, всички инстанции седят зад Load Balancer, as usual. Тук ще се случва аутентикацията и ауторизацията, също ще извършваме координационна дейност, като изпращане на събщения към Message Queue-та, генериране на pre-signed URL-и други.
3. Original Storage - Това ще е BLOB storage, който ще пази видеата на потребителите в оригиналният им формат. Реално погледнато в цялата глава се споменава AWS терминология така че най-вероятно ще е S3.
4. Transcoding Service - Това може би е най-сложната част от цялата система, тук ще encode-ваме видеата в различни формати и резолюции. Това трябва да се случи поради факта, че когато снимаме едно видео, то се запазва под определен формат и с определени качества като bit rate, FPS, резолюция и други. Проблемът е, че не всяко устройство поддържа всички формати и не винаги bandwidth-а на една интернет връзка може да поддържа определен bit rate. В такива ситуации ние искаме да сме адаптивни и да предоставим най-добрият user experience на потребителя, като намалим динамично намалим качеството на видеото повреме на стрийм за да не забива.
5. Encoded Storage - Това ще е отделен storage, който ще пази вече encode-натите видеа.
6. Metadata Database - Това ще базаданни, в която ще събираме мета данни за определено видео, име, големина, продължителност, автор, екстеншън, брой на чънкове и така нататък.
7. Metadata Cache - Тук ще кешираме мета данните на популярни видеа, а може и да е някакъв LRU кеш, понеже очакваме, че по-новите видеа ще бъдат по-популярни.
8. CDN - Ами CDN-ите ще бъдат MVP-то на нашият продукт, целта ни е след всеки ъплоуд да качваме енкоднатите видеа в CDN-ите за да се възползваме от сервиране не видеа по геолокация. Видеата ще се стриймват директно от CDN-а.

Общо взето така изглежда системата на високо ниво. Сега е време да разгледаме как работи Transcoding Service-а. Сам по себе си Transcoding Service-а се дели на различни компоненти:

1. Preprocessor - Този модул има за задача да раздели едно видео на множество чънкове или така наречените GOP (Group of Pictures), по този начин ние можем да продължим енкоудването на видето като паралелно се обработват различните чънкове, също така ако един чък фейлне ние просто можем да retry-нем само него. Също така тук се генрира и DAG-а (Directed Acyclical Graph).
   - DAG model-a е нещо взаимствано от Facebook и общо взето представлява карта за това по какъв начин да се изпълнят определени действия, било то в последователност или паралелно. _Аналогия от истинският живот би било, като се намираш в Плевен и искаш да стигнеш до София да си избереш динамично маршрута в Google Maps, според нуждите ти_. Тоест ако имаме видео, към което потребителя е прикачил thumbnail, ние да направим маршрут, който включва записването на thumbnail-a.
2. DAG scheduler - Този модул взима DAG-а и разпределя таскове поетапно според това дали могат да се случват паралелно или последователно. Тези таскове се пазят в определено Task Queue.
3. Task Workers - Това е pool от worker-и, които ще изпълняват тасковете, които им назначим.
4. Running Queue - Тука ще пазим тасковете, които се изпълняват в момента, заедно в worker-а, който ги изпълнява.
5. Resource Manager - Това е нашият координатор, той върши следните неща:
   1. Периодично проверява task queue-то и взима най-приоритетните
   2. От task worker queue намира подходящ worker за текущият таск и му го назначава
   3. Записва таска и работника в running queue-то
   4. Веднъж щом таска е готов го премахва от рънинг кюто и освобождава работника

- Може би споменах някои от реалните таскове, които трябва да се извършат, но се опасявам, че ми липсва domain knowledge за да кажа точно какво трябва да се случи. Трябва да се направят encoding-ите, да се запишат thumbnail-ите да се събере метадата и да се запише във временен storage. Идеята е, че в момента, в който сме готови обработените видеа се запазват и разпределят по CDN-ите и метдатата се записва.

Общо взето е това, сега трябва да отделим известно време на оптимизации. Първо нека започнем с опит да установим по-ефективни процеси. В момента имаме някакъв последователен процес - Качване на файл -> Енкоуване на оригиналният файл -> Качване на енкоудинга -> Прехвърляне към CDN-a - Ами всичко това могат да са отделни модули, които си комуникират, чрез message queue-та, не се блокират и използваме максимално ресурса си. Другото нещо, което можем да направим е да се възползваме от CDN-ите и да усновим центрове за качване на видеа на стратегически геолокации.
От към сигурност, аз го споменах това, но можем да изградим flow за качване на видеа, чрез pre-signed URL-и, тоест клиента ще заяви на сървъра, че ще качва файл, той създава обект в S3 и връща URL-a с определен expiration date, клиента от там нататък използва УРЛ-а за да качи файла. Другото нещо е да имплементираме някакъв вид encryption (AES, нещо си, не помня) и да използваме някаква система за модериране на сидържание от неподходящ характер като насилие, NSFW съдържание, наредно използване на интелектуална собственост и др. Последната оптимизация би ни била да си спестим някой разход като вместо да кешираме всички видеа в CDN-ите просто запазваме най-популярните, а останлите запазваме и сервираме, чрез dedicated servers.

В wrap up-a можем да обсъдим каква база ще използваме и как ще я скалираме. До колкото прочетох, най-удачно е използваме NoSQL база данни поради факта, че най-вероятно ще имаме ненормализирани данни, които вървят срещу природата на релационните бази данни. Можем да скалираме хоризонтално, тоест да шарднем базата по video ID. Можем да дистрибутираме сървърите в отделни дата центрове по света, имайки предвид, че сме на международният пазар.
